"""
BookRecommender (SBERT Î≤ÑÏ†Ñ, ÏµúÏ¢Ö Ï†ïÎ¶¨Î≥∏)

GoodBooks-10k Ï±Ö Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Í∏∞Î∞òÏúºÎ°ú
Sentence-BERT ÏûÑÎ≤†Îî©ÏùÑ Ïù¥Ïö©Ìïú ÏΩòÌÖêÏ∏† Í∏∞Î∞ò Ï∂îÏ≤úÏùÑ ÏàòÌñâÌïòÎäî ÏóîÏßÑ.

üìå Ìè¨Ìï®Îêú Í∏∞Îä•
------------------------------------------
1) books.csv Î°úÎî© & full_text ÏÉùÏÑ±
2) SBERT ÏûÑÎ≤†Îî© Í≥ÑÏÇ∞ + Ï∫êÏã±
3) LLM Decider Í≤∞Í≥º Í∏∞Î∞ò Ï∂îÏ≤ú
4) exclude_book_ids Ï≤òÎ¶¨
5) Ïû•Î•¥ ÌïÑÌÑ∞ÎßÅ ÌõÑ Ïû¨Ï†ïÎ†¨
6) ÌïúÍµ≠Ïñ¥/ÏïÑÎûçÏñ¥ Îì± ÌäπÏ†ï Ïñ∏Ïñ¥ ÌïÑÌÑ∞ÎßÅ Ï†úÍ±∞

Ïô∏Î∂ÄÏóêÏÑú ÏÇ¨Ïö©ÌïòÎäî ÌïµÏã¨ Î©îÏÑúÎìú
------------------------------------------
- recommend_with_preferences(preference_text, mood_keywords, genres, top_k)
- recommend_from_llm_decision(llm_decision, top_k, user_input, exclude_book_ids)
"""

from __future__ import annotations

import os
import json  # ‚¨ÖÔ∏è ÏÉàÎ°ú Ï∂îÍ∞Ä
from typing import Any, Dict, List, Optional, Set
import logging

import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from src.config import BOOK_TFIDF_MAX_FEATURES


DEFAULT_EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"


# =========================================================
# Ïñ∏Ïñ¥ ÌïÑÌÑ∞: ÏïÑÎûçÏñ¥/ÌûàÎ∏åÎ¶¨Ïñ¥ Ï†úÍ±∞
# =========================================================
def is_non_korean_preferred(book) -> bool:
    title = str(book.get("title", ""))
    for ch in title:
        if '\u0600' <= ch <= '\u06FF':  # Arabic block
            return False
        if '\u0750' <= ch <= '\u077F':  # Arabic supplement
            return False
    return True


# =========================================================
# SBERT ÏΩòÌÖêÏ∏† Í∏∞Î∞ò Ï∂îÏ≤ú ÏóîÏßÑ
# =========================================================
class BookRecommender:
    """
    SBERT ÏûÑÎ≤†Îî© Í∏∞Î∞ò ÏΩòÌÖêÏ∏† Ï∂îÏ≤ú ÏóîÏßÑ.
    """

    def __init__(
        self,
        csv_path: Optional[str] = None,
        embedding_model_name: str = DEFAULT_EMBEDDING_MODEL,
    ) -> None:

        # Í∏∞Î≥∏ books.csv Í≤ΩÎ°ú ÏÑ§Ï†ï
        if csv_path is None:
            base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            csv_path = os.path.join(base_dir, "data", "goodbooks-10k", "books.csv")

        self.csv_path = csv_path
        self.embedding_model_name = embedding_model_name

        # 1) Îç∞Ïù¥ÌÑ∞ Î°úÎìú + Ï†ÑÏ≤òÎ¶¨
        self.df: pd.DataFrame = self._load_and_prepare_df(csv_path)
        
        # 2) TF-IDF Î≤°ÌÑ∞ÎùºÏù¥Ï†Ä + Îß§Ìä∏Î¶≠Ïä§
        self.tfidf_vectorizer: TfidfVectorizer
        self.tfidf_matrix: sparse.spmatrix
        self.tfidf_vectorizer, self.tfidf_matrix = self._build_tfidf_matrix()

        # 3) SBERT Î™®Îç∏ Î°úÎìú
        self.model: SentenceTransformer = SentenceTransformer(self.embedding_model_name)

        # 4) Ï±Ö ÏûÑÎ≤†Îî© ÏÉùÏÑ±/Î°úÎìú
        self.embeddings: np.ndarray = self._build_book_embeddings()

    # --------------------------------------------------------
    # 1. Îç∞Ïù¥ÌÑ∞ Î°úÎî© + Ï†ÑÏ≤òÎ¶¨
    # --------------------------------------------------------
    def _load_and_prepare_df(self, csv_path: str) -> pd.DataFrame:
        # 0) books.csv Î°úÎìú
        df = pd.read_csv(csv_path)

        # ÌîÑÎ°úÏ†ùÌä∏ Î£®Ìä∏ Í∏∞Ï§Ä base_dir
        base_dir = os.path.dirname(
            os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        )

        # 1) ÌïÑÏàò Ïª¨Îüº Î≥¥Ï†ï ----------------------------------------
        if "title" not in df.columns:
            df["title"] = ""
        else:
            df["title"] = df["title"].fillna("").astype(str)

        if "authors" not in df.columns:
            df["authors"] = ""
        else:
            df["authors"] = df["authors"].fillna("").astype(str)

        if "genres" not in df.columns:
            df["genres"] = ""
        else:
            df["genres"] = df["genres"].fillna("").astype(str)

        if "genres_en" not in df.columns:
            df["genres_en"] = ""
        else:
            df["genres_en"] = df["genres_en"].fillna("").astype(str)

        if "description" not in df.columns:
            df["description"] = (
                df["title"].astype(str)
                + " "
                + df["authors"].astype(str)
                + " "
                + df["genres"].astype(str)
            ).str.strip()
        else:
            df["description"] = df["description"].fillna("").astype(str)

        # 2) book_genres.json Î°úÎìúÌï¥ÏÑú genres Î≥¥Í∞ï ------------------
        try:
            genres_json_path = os.path.join(
                base_dir, "data", "goodbooks-10k", "book_genres.json"
            )
            with open(genres_json_path, "r", encoding="utf-8") as f:
                genres_raw = json.load(f)  # {"1": ["fantasy", ...], ...}

            # keyÎ•º int(book_id)Î°ú Îß§Ìïë
            genre_map: Dict[int, List[str]] = {
                int(k): (v or []) for k, v in genres_raw.items()
            }

            def _genres_from_json(bid: Any) -> str:
                try:
                    lst = genre_map.get(int(bid), [])
                except Exception:
                    lst = []
                if not lst:
                    return ""
                return " ".join(str(x) for x in lst)

            df["genres_from_json"] = df["book_id"].map(_genres_from_json).fillna("")
        except Exception as e:
            logging.getLogger(__name__).warning(
                "[BookRec] book_genres.json Î°úÎìú Ïã§Ìå®: %s", e
            )
            df["genres_from_json"] = ""

        # 3) book_tags.csv + tags.csv Î°ú ÌÉúÍ∑∏ ÌÖçÏä§Ìä∏ ÎßåÎì§Í∏∞ ---------
        try:
            tags_path = os.path.join(base_dir, "data", "goodbooks-10k", "tags.csv")
            book_tags_path = os.path.join(
                base_dir, "data", "goodbooks-10k", "book_tags.csv"
            )

            tags_df = pd.read_csv(tags_path)            # tag_id, tag_name
            book_tags_df = pd.read_csv(book_tags_path)  # goodreads_book_id, tag_id, count

            # tag_id -> tag_name
            tag_name_map: Dict[int, str] = dict(
                zip(tags_df["tag_id"].astype(int), tags_df["tag_name"].astype(str))
            )

            book_tags_df["tag_name"] = book_tags_df["tag_id"].map(tag_name_map)
            book_tags_df = book_tags_df[book_tags_df["tag_name"].notna()]

            def _is_meaningful_tag(name: str) -> bool:
                name = str(name).strip()
                if not name:
                    return False
                # Ï†ÑÎ∂Ä Ïà´Ïûê/Í∏∞Ìò∏Î©¥ Î≤ÑÎ¶¨Í∏∞
                if all((not ch.isalpha()) for ch in name):
                    return False
                return True

            book_tags_df = book_tags_df[
                book_tags_df["tag_name"].map(_is_meaningful_tag)
            ]

            # Í∞Å Ï±ÖÎ≥Ñ count Í∏∞Ï§Ä ÏÉÅÏúÑ NÍ∞ú ÌÉúÍ∑∏Îßå ÏÇ¨Ïö©
            TOP_N_TAGS = 5

            # goodreads_book_idÍ∞Ä books.csvÏùò book_idÏôÄ Í∞ôÎã§Í≥† Í∞ÄÏ†ï
            book_tags_df["goodreads_book_id"] = book_tags_df[
                "goodreads_book_id"
            ].astype(int)

            book_tags_df = book_tags_df.sort_values(
                ["goodreads_book_id", "count"], ascending=[True, False]
            )

            top_tags_df = book_tags_df.groupby("goodreads_book_id").head(TOP_N_TAGS)

            tags_agg = (
                top_tags_df.groupby("goodreads_book_id")["tag_name"]
                .apply(lambda xs: " ".join(str(t) for t in xs))
            )

            # df["book_id"] Í∏∞Ï§Ä Îß§Ìïë (book_id == goodreads_book_id Í∞ÄÏ†ï)
            df["tags_text"] = df["book_id"].astype(int).map(tags_agg).fillna("")
        except Exception as e:
            logging.getLogger(__name__).warning(
                "[BookRec] tags/book_tags Î°úÎìú Ïã§Ìå®: %s", e
            )
            df["tags_text"] = ""

        # 4) genres_text + full_text Íµ¨ÏÑ± ---------------------------
        df["genres_text"] = (
            df["genres"].fillna("") + " " + df["genres_from_json"].fillna("")
        ).str.strip()

        df["full_text"] = (
            df["title"].fillna("")
            + " "
            + df["authors"].fillna("")
            + " "
            + df["genres_text"].fillna("")
            + " "
            + df["tags_text"].fillna("")
            + " "
            + df["description"].fillna("")
        ).str.strip()

        # 5) book_genre_text (Ïû•Î•¥/ÌÉúÍ∑∏ Í∏∞Î∞ò boostÏö©) ---------------
        self.book_genre_text: Dict[int, str] = {}
        for _, row in df.iterrows():
            bid = int(row["book_id"])
            meta_text = (
                str(row.get("genres_text", ""))
                + " "
                + str(row.get("tags_text", ""))
                + " "
                + str(row["title"])
                + " "
                + str(row["authors"])
            ).lower()
            self.book_genre_text[bid] = meta_text

        # 6) Ïñ∏Ïñ¥ ÌïÑÌÑ∞ Ï†ÅÏö© -----------------------------------------
        df = df[df.apply(is_non_korean_preferred, axis=1)].reset_index(drop=True)

        # ‚úÖ Î∞òÎìúÏãú dfÎ•º Î∞òÌôòÌï¥Ïïº self.dfÍ∞Ä NoneÏù¥ ÎêòÏßÄ ÏïäÏäµÎãàÎã§.
        return df


    def _build_tfidf_matrix(self) -> tuple[TfidfVectorizer, sparse.spmatrix]:
        """
        full_text Í∏∞Ï§Ä TF-IDF ÌñâÎ†¨ ÏÉùÏÑ±.
        - Ï∫êÏãú ÏóÜÏù¥ Îß§ Ïã§Ìñâ Ïãú Îã§Ïãú ÌïôÏäµ (ÏÜçÎèÑ ÌÅ¨Í≤å Î¨∏Ï†úÎê† Ï†ïÎèÑÎäî ÏïÑÎãò)
        """
        vectorizer = TfidfVectorizer(
            max_features=BOOK_TFIDF_MAX_FEATURES,
            ngram_range=(1, 2),
            stop_words="english",
        )
        texts = self.df["full_text"].tolist()
        tfidf_matrix = vectorizer.fit_transform(texts)
        return vectorizer, tfidf_matrix


    # --------------------------------------------------------
    # 2. SBERT ÏûÑÎ≤†Îî© Î°úÎìú/ÏÉùÏÑ±
    # --------------------------------------------------------
    def _get_embedding_cache_path(self) -> str:
        base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        cache_dir = os.path.join(base_dir, "data", "goodbooks-10k")
        os.makedirs(cache_dir, exist_ok=True)

        tag = self.embedding_model_name.replace("/", "__")
        filename = f"book_embs_{tag}.npy"
        return os.path.join(cache_dir, filename)

    def _build_book_embeddings(self) -> np.ndarray:
        cache_path = self._get_embedding_cache_path()

        # Ï∫êÏãú Î°úÎìú
        if os.path.exists(cache_path):
            try:
                embs = np.load(cache_path)
                if embs.shape[0] == len(self.df):
                    return embs.astype(np.float32)
            except:
                pass  # Ïã§Ìå®ÌïòÎ©¥ ÏÉàÎ°ú Í≥ÑÏÇ∞

        # ÏÉàÎ°ú ÏÉùÏÑ±
        texts = self.df["full_text"].tolist()
        embeddings = self.model.encode(texts, batch_size=64, show_progress_bar=True)
        embeddings = np.asarray(embeddings, dtype=np.float32)

        np.save(cache_path, embeddings)
        return embeddings

    # --------------------------------------------------------
    # 3. ÎÇ¥Î∂Ä Ïú†Ìã∏
    # --------------------------------------------------------
    def _build_query_text(
        self,
        preference_text: Optional[str],
        mood_keywords: Optional[List[str]],
        genres: Optional[List[str]],
    ) -> str:
        tokens = []
        if preference_text:
            tokens.append(preference_text)

        if mood_keywords:
            tokens.extend(mood_keywords)

        if genres:
            tokens.extend(genres)

        return " ".join(tokens).strip()

    def _score_by_embedding(
        self,
        query_text: str,
        top_k: int = 50,
    ) -> List[Dict[str, Any]]:
        """
        SBERT + TF-IDF ÌïòÏù¥Î∏åÎ¶¨Îìú ÏΩòÌÖêÏ∏† Ïä§ÏΩîÏñ¥ÎßÅ.

        - SBERT: self.embeddings (full_text ÏûÑÎ≤†Îî©)
        - TF-IDF: self.tfidf_vectorizer, self.tfidf_matrix (full_text Í∏∞Î∞ò)
        Îëê Ï†êÏàòÎ•º 0.5 : 0.5 Î°ú Îã®Ïàú Í∞ÄÏ§ë ÌèâÍ∑†Ìï©ÎãàÎã§.
        """
        if not query_text:
            return []

        # -----------------------------
        # 1) SBERT similarity
        # -----------------------------
        query_emb = self.model.encode([query_text])
        sims_sbert = cosine_similarity(query_emb, self.embeddings)[0]  # (num_books,)

        # -----------------------------
        # 2) TF-IDF similarity (ÏûàÏúºÎ©¥)
        # -----------------------------
        sims_tfidf = None
        if getattr(self, "tfidf_vectorizer", None) is not None and getattr(self, "tfidf_matrix", None) is not None:
            try:
                q_tfidf = self.tfidf_vectorizer.transform([query_text])
                sims_tfidf = cosine_similarity(q_tfidf, self.tfidf_matrix)[0]  # (num_books,)
            except Exception:
                # ÌòπÏãúÎùºÎèÑ ÏóêÎü¨ ÎÇòÎ©¥ SBERTÎßå ÏÇ¨Ïö©
                sims_tfidf = None

        # -----------------------------
        # 3) Îëê Ïä§ÏΩîÏñ¥ Ìï©ÏπòÍ∏∞
        # -----------------------------
        if sims_tfidf is not None:
            # Í∞ÑÎã®Ìûà 0.5 : 0.5 ÌèâÍ∑†
            sims = 0.5 * sims_sbert + 0.5 * sims_tfidf
        else:
            sims = sims_sbert

        # -----------------------------
        # 4) ÏÉÅÏúÑ top_k ÎΩëÍ∏∞ + 0~1 Ï†ïÍ∑úÌôî
        # -----------------------------
        top_idx = np.argsort(sims)[::-1][:top_k]

        results = []
        for idx in top_idx:
            row = self.df.iloc[idx]
            results.append(
                {
                    "book_id": int(row["book_id"]),
                    "title": str(row["title"]),
                    "authors": str(row["authors"]),
                    "score": float(sims[idx]),
                }
            )

        # 0~1 Ï†ïÍ∑úÌôî
        if results:
            scores = [r["score"] for r in results]
            mx, mn = max(scores), min(scores)
            if mx > mn:
                for r in results:
                    r["score"] = (r["score"] - mn) / (mx - mn)
            else:
                for r in results:
                    r["score"] = 1.0

        return results



    # --------------------------------------------------------
    # 4. Ïû•Î•¥ ÌïÑÌÑ∞ÎßÅ(LLM Ïû•Î•¥ Ïö∞ÏÑ† Ï†ÅÏö©)
    # --------------------------------------------------------
    def _filter_and_reorder_by_genre(
        self,
        results: List[Dict[str, Any]],
        required_genres_en: List[str],
        top_k: int,
        hard_filter_top_n: int = 3,
    ) -> List[Dict[str, Any]]:

        if not results or not required_genres_en:
            return results[:top_k]

        required_genres_en = [g.lower() for g in required_genres_en]

        matched = []
        unmatched = []

        for r in results:
            bid = int(r["book_id"])
            meta = self.book_genre_text.get(bid, "")
            if any(g in meta for g in required_genres_en):
                matched.append(r)
            else:
                unmatched.append(r)

        # Ï∂©Î∂ÑÌïòÎ©¥ matchedÎßå
        if len(matched) >= hard_filter_top_n:
            return matched[:top_k]

        # Î∂ÄÏ°±ÌïòÎ©¥ unmatched ÏÑûÍ∏∞
        out = matched.copy()
        for r in unmatched:
            if len(out) >= top_k:
                break
            out.append(r)

        return out[:top_k]

    # ========================================================
    # 5. Ïô∏Î∂Ä API (ÌïµÏã¨)
    # ========================================================
    def recommend_with_preferences(
        self,
        preference_text: Optional[str],
        mood_keywords: Optional[List[str]],
        genres: Optional[List[str]],
        top_k: int = 50,
    ) -> List[Dict[str, Any]]:

        query = self._build_query_text(preference_text, mood_keywords, genres)
        return self._score_by_embedding(query, top_k)

    def recommend_from_llm_decision(
        self,
        llm_decision: Dict[str, Any],
        top_k: int = 50,
        user_input: Optional[str] = None,
        exclude_book_ids: Optional[Set[int]] = None,
    ) -> List[Dict[str, Any]]:

        if llm_decision is None:
            llm_decision = {}

        exclude_book_ids = exclude_book_ids or set()

        # ------------------------------
        # ‚ë† LLM ÌÜ†ÌÅ∞ Ï°∞Ìï©ÌïòÏó¨ query ÎßåÎì§Í∏∞
        # ------------------------------
        preference_tokens = []

        preference_tokens.extend(llm_decision.get("mentioned_titles", []) or [])
        preference_tokens.extend(llm_decision.get("extra_constraints", []) or [])
        preference_tokens.extend(llm_decision.get("current_emotion", []) or [])
        preference_tokens.extend(llm_decision.get("desired_feeling", []) or [])
        preference_tokens.extend(llm_decision.get("content_mood", []) or [])

        preference_text = " ".join(preference_tokens).strip()
        mood_keywords = llm_decision.get("mood_keywords") or []

        genres_ko = llm_decision.get("genres") or []
        genres_en = llm_decision.get("genres_en") or []
        genres = genres_ko + genres_en

        # ------------------------------
        # ‚ë° SBERTÎ°ú ÎÑâÎÑâÌûà ÌõÑÎ≥¥ ÎΩëÍ∏∞
        # ------------------------------
        base_k = max(top_k * 3, 50)
        raw_results = self.recommend_with_preferences(
            preference_text=preference_text,
            mood_keywords=mood_keywords,
            genres=genres,
            top_k=base_k,
        )

        # ------------------------------
        # ‚ë¢ exclude_book_ids Ï†ÅÏö©
        # ------------------------------
        if exclude_book_ids:
            raw_results = [
                r for r in raw_results if int(r["book_id"]) not in exclude_book_ids
            ]

        # ------------------------------
        # ‚ë£ Ïû•Î•¥ ÌïÑÌÑ∞ÎßÅ (LLM Ïû•Î•¥ Ïö∞ÏÑ†)
        # ------------------------------
        required_genres_en = [g.lower() for g in genres_en if g]
        final = self._filter_and_reorder_by_genre(
            results=raw_results,
            required_genres_en=required_genres_en,
            top_k=top_k,
            hard_filter_top_n=3,
        )

        return final
